{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM4dxtvr6oLQzpx9Ac+Uyj6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christiandatascience/Aprendizaje-por-refuerzo-simple-con-Tensorflow/blob/main/Copia_de_Parte_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Aprendizaje por refuerzo simple con Tensorflow**\n",
        "\n",
        "## Parte 0: Q-Learning con tablas y redes neuronales"
      ],
      "metadata": {
        "id": "oOUJ6YFCnANY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducción\n",
        "\n",
        "Se explorará una familia de algoritmos de Aprendizaje por Refuerzo (RL) llamados algoritmos Q-Learning. Estos son un poco diferentes a los algoritmos basados en políticas, los que se analizarán en las secciones 1 a la 3. \n",
        "\n",
        "En lugar de comenzar con una red neuronal profunda compleja y difícil de manejar, comenzaremos implementando una versión simple de tabla de búsqueda del algoritmo y luego mostraremos cómo implementar un equivalente de red neuronal usando Tensorflow.\n",
        "\n",
        "Lograremos una intuición de lo que realmente está sucediendo en Q-Learning que luego podemos desarrollar en el futuro cuando finalmente combinemos el gradiente de políticas y los enfoques de Q-learning para crear agentes de RL de última generación.\n",
        "\n",
        "A diferencia de los métodos de gradiente de políticas, que intentan aprender funciones que asignan directamente una observación a una acción, Q-Learning intenta aprender el valor de estar en un estado determinado y tomar una acción específica allí. Si bien ambos enfoques en última instancia nos permiten tomar acciones inteligentes dada una situación, los medios para llegar a esa acción difieren significativamente. \n",
        "\n",
        "Es posible que haya oído hablar de DeepQ-Networks, que puede jugar juegos de Atari. En realidad, estas son solo implementaciones más grandes y complejas del algoritmo Q-Learning que vamos a discutir aquí."
      ],
      "metadata": {
        "id": "WVJzbbulniJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-Table Learning"
      ],
      "metadata": {
        "id": "xN3FP6h4olCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "XDIWenvloq-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cargamos el entorno"
      ],
      "metadata": {
        "id": "gnHUnNqoot8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v1')"
      ],
      "metadata": {
        "id": "hXk24Xkoo0xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementamos el algoritmo de aprendizaje Q-Table"
      ],
      "metadata": {
        "id": "noMcJcfJo2Tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize table with all zeros\n",
        "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
        "# Set learning parameters\n",
        "lr = .8\n",
        "y = .95\n",
        "num_episodes = 2000\n",
        "#create lists to contain total rewards and steps per episode\n",
        "#jList = []\n",
        "rList = []\n",
        "for i in range(num_episodes):\n",
        "    #Reset environment and get first new observation\n",
        "    s = env.reset()\n",
        "    rAll = 0\n",
        "    d = False\n",
        "    j = 0\n",
        "    #The Q-Table learning algorithm\n",
        "    while j < 99:\n",
        "        j+=1\n",
        "        #Choose an action by greedily (with noise) picking from Q table\n",
        "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
        "        #Get new state and reward from environment\n",
        "        s1,r,d,_ = env.step(a)\n",
        "        #Update Q-Table with new knowledge\n",
        "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
        "        rAll += r\n",
        "        s = s1\n",
        "        if d == True:\n",
        "            break\n",
        "    #jList.append(j)\n",
        "    rList.append(rAll)"
      ],
      "metadata": {
        "id": "7CL6P4VOo8F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Score over time: \" +  str(sum(rList)/num_episodes))"
      ],
      "metadata": {
        "id": "9GfV2_ZYpATl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Final Q-Table Values\")\n",
        "print (Q)"
      ],
      "metadata": {
        "id": "jjZkn_OwpApt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}